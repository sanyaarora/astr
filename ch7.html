<!doctype html>
<html lang="en">

  <!doctype html>
<html lang="en">

<head>
 <meta charset="utf-8">
  <title> Clustering </title>
  <link rel="icon" type="image/png" href="images/colorcompos.png">
  <meta name="description" content="Hello World">
  <meta name="author" content="Sanya Arora">
  <meta name="description" content="background">

  <meta content="text/html" http-equiv="Content-type" />
  <meta content="width=device-width, initial-scale=1" name="viewport" />

      <link href="https://fonts.googleapis.com/css2?family=Roboto&family=Ubuntu:wght@700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">

   <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
    <link rel="stylesheet" href="css/styles.css">

</head>


</html>
  <!doctype html>
<html lang="en">




<body>

  <nav class="navbar navbar-default">
	  <div class="container-fluid">
	    <div class="navbar-header">
	      <a><img class="logo" src="images/colorcompos.png"> </a>
	     </div>
	    <ul class="nav navbar-nav">
	    	<li> <a href="#" style="color: #2e2a65; font-family: 'Ubuntu', sans-serif; font-size:20px"> ASTR 408 Final Project</a> </li>
	      <li><a href ="index.html"> Home </a></li>
	      <li class="dropdown">
	        <a class="dropdown-toggle" data-toggle="dropdown" href="#">Course Summary
	        <span class="caret"></span></a>
	        <ul class="dropdown-menu">


	          <li><a href=ch1.html> Probability Concepts </a></li>
	          <li><a href=ch2.html> Nonparametrics </a></li>
	          <li><a href=ch3.html> Regression </a></li>
            <li><a href=ch4.html> Smoothing </a></li>
            <li><a href=ch5.html> Multivariate Analysis </a></li>
            <li><a href=ch6.html> Time Series </a></li>
            <li><a href=ch7.html> Clustering </a></li>
            <li><a href=ch8.html> Truncated Data </a></li>
            <li><a href=ch9.html> Spatial Processes </a></li>


	        </ul>
	      </li>
	    </ul>
	  </div>
	</nav>



</body>
</html>

<body>

  <h2> Clustering (Chapter 9) </h2>
      <div class = "container-fluid">
        <div class ="col-lg-12">
          <div class="jumbotron vertical-center" style="background-color: #f2e6dc">
            <p class="text"> We discussed clustering in this section of the course. Clustering is important when we have data that
              we must classify in separate categories, or we want to see if certain populations in data are signficantly far apart.
              <br>
              <br>
              Dendograms are diagrams that visually represent the clusters produced by hierarchical
              clustering. Dendrograms show how individual elements or clusters are merged or divided based on the distance metric
              used in clustering. Agglomerative hierarchical clustering is a common clustering method in which each data point is
              a separate cluster
              and then merged them based on the distance between clusters. Some common forms of Agglomerative hierarchical clustering are:
               <br> 
              <b>  Single Linkage Clustering: </b> Also known as the nearest neighbor clustering. Merges clusters with the smallest minimum pairwise distance.
                <br>
              <b> Complete Linkage Clustering: </b>
                Merges clusters based on the largest distance between any member of the clusters, aiming for tightly knit clusters.
                <br>
              <b> Average Linkage Clustering: </b>
              Uses the average distance between all members of clusters to merge different clusters.
              <br>
              <b> Wardâ€™s Minimum Variance: </b> Minimizes the total variance within each cluster, so each merge increases the
              total variance the least amount possible.
              <br>
              <br>
              The most common nonheirarchical clustering method is k-means clustering. K-means partitions data into k clusters by
              assigning points to the cluster center nearest to them, and then re-calculating cluster centers based on the assigned points.
              <br>
              <br>
              Unlike unsupervised clustering, supervised clustering is a different techinque that involves training models on a
              labeled dataset, and then using the trained model to classify new data points. Some supervised classification methods are:
              <br>
              <b> Linear Discriminant Analysis (LDA) and Classification Trees: </b>
              LDA finds a linear combination of features that separates multiple classes of objects.
              Classification trees split data into branches to form decision nodes, which help in making predictions.
              <br>
              <b> Nearest-Neighbor Classifiers: </b>
              Classify new data points based on the most common cluster among the points' nearest k neighbors.
              <br>
              <b> Automated Neural Networks: </b> Use learning algorithms to identify patterns, and then classify data
              based on the learned weights.
              <br>
              <br>
              Clustering has many applications whenever we want to classify data. For example, my research involves classifying
              data at the XENONnT experiment into either electron or photon signals. We use unsuperivised clustering to achieve this.

















            </p>





      </div>
    </div>
  </div>

<footer>
<html>


</html>
</footer>
</body>

</html>
